# coding=utf-8

""" count_contexts_with_re.py counts all kmers in the intronic sequence generated by
    count_intronic_sites.py and stores the results in a Counter object.

    Parameters are job number, input filename, number of jobs and directory.

    Sample run statement is:
    nohup python3 ~/helmutsimonpython/mutationprobvariance/probpoly/count_contexts_with_re.py ccr001
    intronic_sequencehumancis_ch1_002.pklz -j 5 > ccr001.txt &"""


import os
from time import time
import gzip
import pickle
import itertools
import re
from joblib import Parallel, delayed
import click
from scitrack import CachingLogger, get_file_hexdigest

LOGGER = CachingLogger(create_dir=True)

def count_single_context(context, sequence):
    mcount = 0
    matches = re.finditer(context, sequence)
    for match in matches:
        mcount += 1
    return mcount

def main_core(job_no, filename=None, n_jobs=5, context_size=9, dir='data'):
    #global sequence
    if not os.path.exists(dir):
        os.makedirs(dir)
    LOGGER.log_file_path = dir + "/" + str(os.path.basename(__file__)) + '_' + job_no + ".log"
    start_time = time()
    LOGGER.log_args()
    LOGGER.log_message(get_file_hexdigest(__file__), label="Hex digest of script.".ljust(30))
    LOGGER.log_message('Name = ' + re.__name__ + ', version = ' + re.__version__,
                       label="Imported module".ljust(30))
    infile = open(filename, 'r')
    LOGGER.input_file(infile.name)
    infile.close()
    with gzip.open(filename, 'rb') as sequence:  # i.e. intron sequence
        sequence = pickle.load(sequence)
    context_size = int(context_size)
    contexts_generator = itertools.product('ACGT', repeat=context_size)
    contexts = tuple(''.join(context) for context in contexts_generator)
    concounts = list()
    for context in contexts:
        concount = count_single_context(context, sequence)
        concounts.append(concount)
    #concounts = Parallel(n_jobs=n_jobs)(delayed(count_single_context)(context) for context in contexts)
    context_dict = dict(zip(contexts, concounts))

    outfile_name = dir + '/context_dict_' + job_no + '.pklz'
    with gzip.open(outfile_name, 'wb') as outfile:
        pickle.dump(context_dict, outfile)
    outfile = open(outfile_name, 'r')
    LOGGER.output_file(outfile.name)
    outfile.close()
    LOGGER.log_message(str(len(context_dict.keys())), label="Number of dictionary keys".ljust(30))
    LOGGER.log_message(str(sum(context_dict.values())), label="Count of contexts".ljust(30))
    duration = time() - start_time
    LOGGER.log_message("%.2f" % (duration / 60.), label="run duration (minutes)".ljust(30))


@click.command()
@click.argument('job_no')
@click.argument('filename')
@click.option('-j', '--n_jobs', default=5, type=int, help='Number of parallel jobs. Default is 5.')
@click.option('-s', '--context_size', default=9, type=int, help='Size of contexts to be counted. Default is 9.')
@click.option('-d', '--dir', default='data', type=click.Path(),
              help='Directory name for data and log files. Defaults is data')
def main(job_no, filename, n_jobs, context_size, dir):
    main_core(job_no, filename, n_jobs, context_size, dir)


if __name__ == "__main__":
    main()
